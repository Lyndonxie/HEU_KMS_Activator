{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lyndonxie/HEU_KMS_Activator/blob/master/colab_webui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "环境配置 environment"
      ],
      "metadata": {
        "id": "_o6a8GS2lWQM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9b7iFV3dm1f"
      },
      "source": [
        "!pip install -q condacolab\n",
        "# Setting up condacolab and installing packages\n",
        "import condacolab\n",
        "condacolab.install_from_url(\"https://repo.anaconda.com/miniconda/Miniconda3-py39_23.11.0-2-Linux-x86_64.sh\")\n",
        "%cd -q /content\n",
        "!git clone https://github.com/RVC-Boss/GPT-SoVITS\n",
        "!conda install -y -q -c pytorch -c nvidia cudatoolkit\n",
        "%cd -q /content/GPT-SoVITS\n",
        "!conda install -y -q -c conda-forge gcc gxx ffmpeg cmake -c pytorch -c nvidia\n",
        "!/usr/local/bin/pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download pretrained models 下载预训练模型\n",
        "!mkdir -p /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\n",
        "!mkdir -p /content/GPT-SoVITS/tools/damo_asr/models\n",
        "!mkdir -p /content/GPT-SoVITS/tools/uvr5\n",
        "%cd /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\n",
        "!git clone https://huggingface.co/lj1995/GPT-SoVITS\n",
        "%cd /content/GPT-SoVITS/tools/damo_asr/models\n",
        "!git clone https://www.modelscope.cn/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch.git\n",
        "!git clone https://www.modelscope.cn/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch.git\n",
        "!git clone https://www.modelscope.cn/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch.git\n",
        "# @title UVR5 pretrains 安装uvr5模型\n",
        "%cd /content/GPT-SoVITS/tools/uvr5\n",
        "%rm -r uvr5_weights\n",
        "!git clone https://huggingface.co/Delik/uvr5_weights\n",
        "!git config core.sparseCheckout true\n",
        "!mv /content/GPT-SoVITS/GPT_SoVITS/pretrained_models/GPT-SoVITS/* /content/GPT-SoVITS/GPT_SoVITS/pretrained_models/"
      ],
      "metadata": {
        "id": "0NgxXg5sjv7z",
        "outputId": "31b2f1d2-a84e-4d98-a003-60298a928e8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Total 466 (delta 40), reused 60 (delta 32), pack-reused 395\u001b[K\n",
            "Receiving objects: 100% (466/466), 1.12 GiB | 8.85 MiB/s, done.\n",
            "Resolving deltas: 100% (268/268), done.\n",
            "remote: Total 466 (delta 40), reused 60 (delta 32), pack-reused 395\u001b[K\n",
            "Receiving objects: 100% (466/466), 1.12 GiB | 8.85 MiB/s, done.\n",
            "Resolving deltas: 100% (268/268), done.\n",
            "Cloning into 'speech_fsmn_vad_zh-cn-16k-common-pytorch'...\n",
            "Cloning into 'speech_fsmn_vad_zh-cn-16k-common-pytorch'...\n",
            "remote: Enumerating objects: 184, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Enumerating objects: 184, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 184 (delta 18), reused 23 (delta 10), pack-reused 145\u001b[K\n",
            "Receiving objects: 100% (184/184), 4.66 MiB | 20.13 MiB/s, done.\n",
            "Resolving deltas: 100% (91/91), done.\n",
            "remote: Total 184 (delta 18), reused 23 (delta 10), pack-reused 145\u001b[K\n",
            "Receiving objects: 100% (184/184), 4.66 MiB | 20.13 MiB/s, done.\n",
            "Resolving deltas: 100% (91/91), done.\n",
            "Cloning into 'punc_ct-transformer_zh-cn-common-vocab272727-pytorch'...\n",
            "Cloning into 'punc_ct-transformer_zh-cn-common-vocab272727-pytorch'...\n",
            "remote: Enumerating objects: 170, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Enumerating objects: 170, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 170 (delta 25), reused 30 (delta 14), pack-reused 120\u001b[K\n",
            "Receiving objects: 100% (170/170), 257.56 MiB | 34.93 MiB/s, done.\n",
            "Resolving deltas: 100% (86/86), done.\n",
            "remote: Total 170 (delta 25), reused 30 (delta 14), pack-reused 120\u001b[K\n",
            "Receiving objects: 100% (170/170), 257.56 MiB | 34.93 MiB/s, done.\n",
            "Resolving deltas: 100% (86/86), done.\n",
            "/content/GPT-SoVITS/tools/uvr5\n",
            "/content/GPT-SoVITS/tools/uvr5\n",
            "Cloning into 'uvr5_weights'...\n",
            "Cloning into 'uvr5_weights'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 15 (delta 1), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (15/15), 3.25 KiB | 1.08 MiB/s, done.\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 15 (delta 1), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (15/15), 3.25 KiB | 1.08 MiB/s, done.\n",
            "Filtering content: 100% (9/9), 594.44 MiB | 67.54 MiB/s, done.\n",
            "Filtering content: 100% (9/9), 594.44 MiB | 67.54 MiB/s, done.\n",
            "/content/GPT-SoVITS/GPT_SoVITS/pretrained_models\n",
            "fatal: destination path 'GPT-SoVITS' already exists and is not an empty directory.\n",
            "/content/GPT-SoVITS/tools/damo_asr/models\n",
            "fatal: destination path 'speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch' already exists and is not an empty directory.\n",
            "fatal: destination path 'speech_fsmn_vad_zh-cn-16k-common-pytorch' already exists and is not an empty directory.\n",
            "fatal: destination path 'punc_ct-transformer_zh-cn-common-vocab272727-pytorch' already exists and is not an empty directory.\n",
            "/content/GPT-SoVITS/tools/uvr5\n",
            "Cloning into 'uvr5_weights'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 15 (delta 1), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (15/15), 3.25 KiB | 476.00 KiB/s, done.\n",
            "Filtering content: 100% (9/9), 594.44 MiB | 117.24 MiB/s, done.\n",
            "mv: cannot stat '/content/GPT-SoVITS/GPT_SoVITS/pretrained_models/GPT-SoVITS/*': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title launch WebUI 启动WebUI\n",
        "!/usr/local/bin/pip install ipykernel\n",
        "!sed -i '10s/False/True/' /content/GPT-SoVITS/config.py\n",
        "%cd /content/GPT-SoVITS/\n",
        "!/usr/local/bin/python  webui.py"
      ],
      "metadata": {
        "id": "4oRGUzkrk8C7",
        "outputId": "8ba0a9b9-4756-4ec8-eba3-465c4ce887b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipykernel\n",
            "  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting comm>=0.1.1 (from ipykernel)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting debugpy>=1.6.5 (from ipykernel)\n",
            "  Downloading debugpy-1.8.7-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting ipython>=7.23.1 (from ipykernel)\n",
            "  Downloading ipython-8.18.1-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel)\n",
            "  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting jupyter-core!=5.0.*,>=4.12 (from ipykernel)\n",
            "  Downloading jupyter_core-5.7.2-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting matplotlib-inline>=0.1 (from ipykernel)\n",
            "  Downloading matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting nest-asyncio (from ipykernel)\n",
            "  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/site-packages (from ipykernel) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/site-packages (from ipykernel) (6.1.0)\n",
            "Collecting pyzmq>=24 (from ipykernel)\n",
            "  Downloading pyzmq-26.2.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting tornado>=6.1 (from ipykernel)\n",
            "  Downloading tornado-6.4.1-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting traitlets>=5.4.0 (from ipykernel)\n",
            "  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting prompt-toolkit<3.1.0,>=3.0.41 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading prompt_toolkit-3.0.48-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (2.18.0)\n",
            "Collecting stack-data (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (4.12.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (1.2.2)\n",
            "Collecting pexpect>4.3 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.3 in /usr/local/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel) (8.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (3.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.9/site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel) (3.20.2)\n",
            "Collecting parso<0.9.0,>=0.8.3 (from jedi>=0.16->ipython>=7.23.1->ipykernel)\n",
            "  Downloading parso-0.8.4-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting ptyprocess>=0.5 (from pexpect>4.3->ipython>=7.23.1->ipykernel)\n",
            "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting wcwidth (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel)\n",
            "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n",
            "Collecting executing>=1.2.0 (from stack-data->ipython>=7.23.1->ipykernel)\n",
            "  Downloading executing-2.1.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting asttokens>=2.1.0 (from stack-data->ipython>=7.23.1->ipykernel)\n",
            "  Downloading asttokens-2.4.1-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting pure-eval (from stack-data->ipython>=7.23.1->ipykernel)\n",
            "  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Downloading ipykernel-6.29.5-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading debugpy-1.8.7-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipython-8.18.1-py3-none-any.whl (808 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m808.2/808.2 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_core-5.7.2-py3-none-any.whl (28 kB)\n",
            "Downloading matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)\n",
            "Downloading pyzmq-26.2.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (862 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m862.1/862.1 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tornado-6.4.1-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.8/436.8 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
            "Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prompt_toolkit-3.0.48-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
            "Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading executing-2.1.0-py2.py3-none-any.whl (25 kB)\n",
            "Downloading parso-0.8.4-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.7/103.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Downloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
            "Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
            "Installing collected packages: wcwidth, pure-eval, ptyprocess, traitlets, tornado, pyzmq, prompt-toolkit, pexpect, parso, nest-asyncio, executing, debugpy, asttokens, stack-data, matplotlib-inline, jupyter-core, jedi, comm, jupyter-client, ipython, ipykernel\n",
            "Successfully installed asttokens-2.4.1 comm-0.2.2 debugpy-1.8.7 executing-2.1.0 ipykernel-6.29.5 ipython-8.18.1 jedi-0.19.1 jupyter-client-8.6.3 jupyter-core-5.7.2 matplotlib-inline-0.1.7 nest-asyncio-1.6.0 parso-0.8.4 pexpect-4.9.0 prompt-toolkit-3.0.48 ptyprocess-0.7.0 pure-eval-0.2.3 pyzmq-26.2.0 stack-data-0.6.3 tornado-6.4.1 traitlets-5.14.3 wcwidth-0.2.13\n",
            "/content/GPT-SoVITS\n",
            "Downloading g2pw model...\n",
            "Extracting g2pw model...\n",
            "Running on local URL:  http://0.0.0.0:9874\n",
            "Running on public URL: https://1428dd1e9ae5238a18.gradio.live\n",
            "\"/usr/local/bin/python\" tools/slice_audio.py \"/content/yunjian\" \"output/slicer_opt\" -34 4000 300 10 500 0.9 0.25 0 4\n",
            "\"/usr/local/bin/python\" tools/slice_audio.py \"/content/yunjian\" \"output/slicer_opt\" -34 4000 300 10 500 0.9 0.25 1 4\n",
            "\"/usr/local/bin/python\" tools/slice_audio.py \"/content/yunjian\" \"output/slicer_opt\" -34 4000 300 10 500 0.9 0.25 2 4\n",
            "\"/usr/local/bin/python\" tools/slice_audio.py \"/content/yunjian\" \"output/slicer_opt\" -34 4000 300 10 500 0.9 0.25 3 4\n",
            "执行完毕，请检查输出文件\n",
            "执行完毕，请检查输出文件\n",
            "执行完毕，请检查输出文件\n",
            "执行完毕，请检查输出文件\n",
            "\"/usr/local/bin/python\" tools/asr/fasterwhisper_asr.py -i \"output/slicer_opt\" -o \"output/asr_opt\" -s large-v3 -l auto -p float32\n",
            "loading faster whisper model: large-v3 large-v3\n",
            "model.bin:   0% 10.5M/3.09G [00:00<01:24, 36.2MB/s]\n",
            "config.json:   0% 0.00/635 [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "config.json: 2.39kB [00:00, 184kB/s]      \n",
            "preprocessor_config.json: 100% 340/340 [00:00<00:00, 49.3kB/s]\n",
            "\n",
            "model.bin:   1% 21.0M/3.09G [00:00<01:16, 40.0MB/s]\n",
            "model.bin:   1% 31.5M/3.09G [00:00<01:21, 37.5MB/s]\n",
            "model.bin:   1% 41.9M/3.09G [00:01<01:10, 43.3MB/s]\n",
            "vocabulary.json: 276kB [00:00, 447kB/s]\u001b[A\n",
            "\n",
            "tokenizer.json: 0.00B [00:00, ?B/s]\u001b[A\u001b[A\n",
            "model.bin:   2% 52.4M/3.09G [00:01<01:14, 40.7MB/s]\n",
            "\n",
            "tokenizer.json: 305kB [00:00, 1.38MB/s]\u001b[A\u001b[A\n",
            "vocabulary.json: 1.07MB [00:01, 958kB/s]\n",
            "model.bin:   2% 62.9M/3.09G [00:01<01:09, 43.5MB/s]\n",
            "\n",
            "model.bin:   2% 73.4M/3.09G [00:01<01:09, 43.6MB/s]\n",
            "\n",
            "tokenizer.json: 528kB [00:00, 716kB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.bin:   3% 83.9M/3.09G [00:02<01:10, 42.8MB/s]\n",
            "\n",
            "model.bin:   3% 94.4M/3.09G [00:02<01:09, 43.1MB/s]\n",
            "\n",
            "tokenizer.json: 1.24MB [00:01, 1.39MB/s]\u001b[A\u001b[A\n",
            "\n",
            "tokenizer.json: 2.48MB [00:01, 1.86MB/s]\n",
            "model.bin: 100% 3.09G/3.09G [01:12<00:00, 42.4MB/s]\n",
            "  0% 0/5 [00:00<?, ?it/s]Unable to load any of {libcudnn_ops.so.9.1.0, libcudnn_ops.so.9.1, libcudnn_ops.so.9, libcudnn_ops.so}\n",
            "Invalid handle. Cannot load symbol cudnnCreateTensorDescriptor\n",
            "Aborted (core dumped)\n",
            "\n",
            "\n",
            "\n",
            "\"/usr/local/bin/python\" tools/asr/funasr_asr.py -i \"output/slicer_opt\" -o \"output/asr_opt\" -s large -l zh -p float32\n",
            "2024-10-30 08:25:48,266 - modelscope - INFO - PyTorch version 2.5.1 Found.\n",
            "2024-10-30 08:25:48,268 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
            "2024-10-30 08:25:48,268 - modelscope - INFO - No valid ast index found from /root/.cache/modelscope/ast_indexer, generating ast index from prebuilt!\n",
            "2024-10-30 08:25:48,361 - modelscope - INFO - Loading done! Current index file version is 1.10.0, with md5 176a406b62b0e88477b330749528879b and a total number of 946 components indexed\n",
            "2024-10-30 08:25:50,492 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "Downloading: 100% 10.9k/10.9k [00:00<00:00, 1.12MB/s]\n",
            "Downloading: 100% 173k/173k [00:00<00:00, 4.67MB/s]\n",
            "Downloading: 100% 2.45k/2.45k [00:00<00:00, 924kB/s]\n",
            "Downloading: 100% 472/472 [00:00<00:00, 173kB/s]\n",
            "Downloading: 100% 840M/840M [00:15<00:00, 57.9MB/s]\n",
            "Downloading: 100% 19.1k/19.1k [00:00<00:00, 2.45MB/s]\n",
            "Downloading: 100% 7.90M/7.90M [00:00<00:00, 18.3MB/s]\n",
            "Downloading: 100% 48.7k/48.7k [00:00<00:00, 2.95MB/s]\n",
            "Downloading: 100% 91.5k/91.5k [00:00<00:00, 5.27MB/s]\n",
            "ckpt: /root/.cache/modelscope/hub/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/model.pt\n",
            "/usr/local/lib/python3.9/site-packages/funasr/train_utils/load_pretrained_model.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  src_state = torch.load(path, map_location=map_location)\n",
            "2024-10-30 08:26:30,853 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "Downloading: 100% 7.85k/7.85k [00:00<00:00, 873kB/s]\n",
            "Downloading: 100% 1.19k/1.19k [00:00<00:00, 452kB/s]\n",
            "Downloading: 100% 365/365 [00:00<00:00, 143kB/s]\n",
            "Downloading: 100% 1.64M/1.64M [00:00<00:00, 21.5MB/s]\n",
            "Downloading: 100% 8.45k/8.45k [00:00<00:00, 2.92MB/s]\n",
            "Downloading: 100% 27.3k/27.3k [00:00<00:00, 3.39MB/s]\n",
            "Downloading: 100% 2.16M/2.16M [00:00<00:00, 19.8MB/s]\n",
            "ckpt: /root/.cache/modelscope/hub/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch/model.pt\n",
            "2024-10-30 08:26:42,190 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "Downloading: 100% 6.00k/6.00k [00:00<00:00, 2.06MB/s]\n",
            "Downloading: 100% 810/810 [00:00<00:00, 282kB/s]\n",
            "Downloading: 100% 373/373 [00:00<00:00, 125kB/s]\n",
            "Downloading: 100% 278M/278M [00:08<00:00, 36.0MB/s]\n",
            "Downloading: 100% 863/863 [00:00<00:00, 329kB/s]\n",
            "Downloading: 100% 11.2k/11.2k [00:00<00:00, 4.23MB/s]\n",
            "Downloading: 100% 151k/151k [00:00<00:00, 6.06MB/s]\n",
            "Downloading: 100% 4.01M/4.01M [00:00<00:00, 27.7MB/s]\n",
            "ckpt: /root/.cache/modelscope/hub/iic/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/model.pt\n",
            "FunASR 模型加载完成: ZH\n",
            "  0% 0/5 [00:00<?, ?it/s]\n",
            "yunjian.mp3_0000000000_0000143680.wav\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100% 1/1 [00:00<00:00,  1.15it/s]\u001b[A\n",
            "{'load_data': '0.341', 'extract_feat': '0.107', 'forward': '0.867', 'batch_size': '1', 'rtf': '0.194'}, : 100% 1/1 [00:00<00:00,  1.15it/s]\u001b[A\n",
            "rtf_avg: 0.194: 100% 1/1 [00:00<00:00,  1.15it/s]\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A/usr/local/lib/python3.9/site-packages/funasr/models/paraformer/model.py:249: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(False):\n",
            "/usr/local/lib/python3.9/site-packages/funasr/models/paraformer/cif_predictor.py:212: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(False):\n",
            "\n",
            "\n",
            "100% 1/1 [00:01<00:00,  1.86s/it]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': '0.000', 'extract_feat': '0.008', 'forward': '1.864', 'batch_size': '1', 'rtf': '0.414'}, : 100% 1/1 [00:01<00:00,  1.86s/it]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: 0.414: 100% 1/1 [00:01<00:00,  1.87s/it]\n",
            "\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.053', 'batch_size': '1', 'rtf': '-0.053'}, : 100% 1/1 [00:00<00:00, 18.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: -0.053: 100% 1/1 [00:00<00:00, 18.75it/s]\n",
            "\n",
            "100% 1/1 [00:01<00:00,  1.94s/it]\u001b[A\n",
            "rtf_avg: 0.428, time_speech:  4.490, time_escape: 1.924: 100% 1/1 [00:01<00:00,  1.94s/it]\n",
            " 20% 1/5 [00:02<00:11,  2.81s/it]\n",
            "yunjian.mp3_0000143680_0000357120.wav\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "{'load_data': '0.027', 'extract_feat': '0.013', 'forward': '0.099', 'batch_size': '1', 'rtf': '0.015'}, : 100% 1/1 [00:00<00:00, 10.12it/s]\u001b[A\n",
            "rtf_avg: 0.015: 100% 1/1 [00:00<00:00, 10.04it/s]\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 1/1 [00:00<00:00,  6.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': '0.000', 'extract_feat': '0.011', 'forward': '0.146', 'batch_size': '1', 'rtf': '0.022'}, : 100% 1/1 [00:00<00:00,  6.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: 0.022: 100% 1/1 [00:00<00:00,  6.81it/s]\n",
            "\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.014', 'batch_size': '1', 'rtf': '-0.014'}, : 100% 1/1 [00:00<00:00, 72.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: -0.014: 100% 1/1 [00:00<00:00, 71.51it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00,  5.56it/s]\u001b[A\n",
            "rtf_avg: 0.025, time_speech:  6.670, time_escape: 0.166: 100% 1/1 [00:00<00:00,  5.55it/s]\n",
            " 40% 2/5 [00:03<00:03,  1.32s/it]\n",
            "yunjian.mp3_0000357120_0000508480.wav\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "{'load_data': '0.020', 'extract_feat': '0.012', 'forward': '0.077', 'batch_size': '1', 'rtf': '0.016'}, : 100% 1/1 [00:00<00:00, 12.89it/s]\u001b[A\n",
            "rtf_avg: 0.016: 100% 1/1 [00:00<00:00, 12.79it/s]\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 1/1 [00:00<00:00,  9.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': '0.000', 'extract_feat': '0.008', 'forward': '0.100', 'batch_size': '1', 'rtf': '0.021'}, : 100% 1/1 [00:00<00:00,  9.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: 0.021: 100% 1/1 [00:00<00:00,  9.87it/s]\n",
            "\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.014', 'batch_size': '1', 'rtf': '-0.014'}, : 100% 1/1 [00:00<00:00, 73.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: -0.014: 100% 1/1 [00:00<00:00, 72.21it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00,  7.49it/s]\u001b[A\n",
            "rtf_avg: 0.025, time_speech:  4.730, time_escape: 0.120: 100% 1/1 [00:00<00:00,  7.48it/s]\n",
            " 60% 3/5 [00:03<00:01,  1.23it/s]\n",
            "yunjian.mp3_0000508480_0000732800.wav\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100% 1/1 [00:00<00:00,  9.05it/s]\u001b[A\n",
            "{'load_data': '0.016', 'extract_feat': '0.013', 'forward': '0.110', 'batch_size': '1', 'rtf': '0.016'}, : 100% 1/1 [00:00<00:00,  9.05it/s]\u001b[A\n",
            "rtf_avg: 0.016: 100% 1/1 [00:00<00:00,  8.98it/s]\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 1/1 [00:00<00:00,  8.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': '0.000', 'extract_feat': '0.010', 'forward': '0.112', 'batch_size': '1', 'rtf': '0.016'}, : 100% 1/1 [00:00<00:00,  8.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: 0.016: 100% 1/1 [00:00<00:00,  8.83it/s]\n",
            "\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.014', 'batch_size': '1', 'rtf': '-0.014'}, : 100% 1/1 [00:00<00:00, 71.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: -0.014: 100% 1/1 [00:00<00:00, 69.76it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00,  6.77it/s]\u001b[A\n",
            "rtf_avg: 0.019, time_speech:  7.010, time_escape: 0.133: 100% 1/1 [00:00<00:00,  6.75it/s]\n",
            " 80% 4/5 [00:03<00:00,  1.68it/s]\n",
            "yunjian.mp3_0000732800_0000815040.wav\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "{'load_data': '0.014', 'extract_feat': '0.007', 'forward': '0.046', 'batch_size': '1', 'rtf': '0.018'}, : 100% 1/1 [00:00<00:00, 21.74it/s]\u001b[A\n",
            "rtf_avg: 0.018: 100% 1/1 [00:00<00:00, 21.48it/s]\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 1/1 [00:00<00:00,  7.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': '0.000', 'extract_feat': '0.006', 'forward': '0.126', 'batch_size': '1', 'rtf': '0.049'}, : 100% 1/1 [00:00<00:00,  7.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: 0.049: 100% 1/1 [00:00<00:00,  7.87it/s]\n",
            "\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.006', 'batch_size': '1', 'rtf': '-0.006'}, : 100% 1/1 [00:00<00:00, 156.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: -0.006: 100% 1/1 [00:00<00:00, 151.25it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00,  6.54it/s]\u001b[A\n",
            "rtf_avg: 0.054, time_speech:  2.570, time_escape: 0.139: 100% 1/1 [00:00<00:00,  6.53it/s]\n",
            "100% 5/5 [00:03<00:00,  1.33it/s]\n",
            "ASR 任务完成->标注文件路径: /content/GPT-SoVITS/output/asr_opt/slicer_opt.list\n",
            "\n",
            "\"/usr/local/bin/python\" tools/subfix_webui.py --load_list \"/content/GPT-SoVITS/output/asr_opt/slicer_opt.list\" --webui_port 9871 --is_share True\n",
            "Running on local URL:  http://0.0.0.0:9871\n",
            "Running on public URL: https://6ea682d51316663fef.gradio.live\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/1-get-text.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/1-get-text.py\n",
            "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "yunjian.mp3_0000000000_0000143680.wav\n",
            "yunjian.mp3_0000143680_0000357120.wav\n",
            "当前使用g2pw进行拼音推理\n",
            "当前使用g2pw进行拼音推理\n",
            "Building prefix dict from the default dictionary ...\n",
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "Loading model cost 2.150 seconds.\n",
            "Prefix dict has been built succesfully.\n",
            "Dumping model to file cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "Loading model cost 1.998 seconds.\n",
            "Prefix dict has been built succesfully.\n",
            "yunjian.mp3_0000357120_0000508480.wav\n",
            "yunjian.mp3_0000508480_0000732800.wav\n",
            "yunjian.mp3_0000732800_0000815040.wav\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/2-get-hubert-wav32k.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/2-get-hubert-wav32k.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/3-get-semantic.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/3-get-semantic.py\n",
            "<All keys matched successfully>\n",
            "<All keys matched successfully>\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/s2_train.py --config \"/content/GPT-SoVITS/TEMP/tmp_s2.json\"\n",
            "phoneme_data_len: 5\n",
            "wav_data_len: 100\n",
            "100% 100/100 [00:00<00:00, 81984.05it/s]\n",
            "skipped_phone:  0 , skipped_dur:  0\n",
            "total left:  100\n",
            "<All keys matched successfully>\n",
            "<All keys matched successfully>\n",
            "  0% 0/15 [00:00<?, ?it/s]\n",
            "\n",
            "[rank0]:[W1030 08:34:18.542046610 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
            "100% 15/15 [01:01<00:00,  4.07s/it]\n",
            "100% 15/15 [00:16<00:00,  1.08s/it]\n",
            "100% 15/15 [00:17<00:00,  1.18s/it]\n",
            "100% 15/15 [00:18<00:00,  1.24s/it]\n",
            "100% 15/15 [00:18<00:00,  1.26s/it]\n",
            "100% 15/15 [00:19<00:00,  1.28s/it]\n",
            "100% 15/15 [00:18<00:00,  1.25s/it]\n",
            "100% 15/15 [00:15<00:00,  1.02s/it]\n",
            "\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/s1_train.py --config_file \"/content/GPT-SoVITS/TEMP/tmp_s1.yaml\" \n",
            "Seed set to 1234\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "/content/GPT-SoVITS/GPT_SoVITS/AR/models/t2s_lightning_module.py:26: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "<All keys matched successfully>\n",
            "ckpt_path: None\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "semantic_data_len: 5\n",
            "phoneme_data_len: 5\n",
            "                               item_name                                     semantic_audio\n",
            "0  yunjian.mp3_0000000000_0000143680.wav  520 280 280 72 937 295 403 93 466 789 790 370 ...\n",
            "1  yunjian.mp3_0000357120_0000508480.wav  368 650 911 97 682 652 745 12 88 787 530 173 4...\n",
            "2  yunjian.mp3_0000732800_0000815040.wav  910 368 300 34 836 894 393 890 479 879 893 85 ...\n",
            "3  yunjian.mp3_0000143680_0000357120.wav  722 272 296 491 51 423 688 326 506 625 699 893...\n",
            "4  yunjian.mp3_0000508480_0000732800.wav  722 1003 187 889 94 609 543 891 314 604 562 94...\n",
            "dataset.__len__(): 100\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                 | Params | Mode \n",
            "-------------------------------------------------------\n",
            "0 | model | Text2SemanticDecoder | 77.6 M | train\n",
            "-------------------------------------------------------\n",
            "77.6 M    Trainable params\n",
            "0         Non-trainable params\n",
            "77.6 M    Total params\n",
            "310.426   Total estimated model params size (MB)\n",
            "257       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/content/GPT-SoVITS/GPT_SoVITS/AR/data/dataset.py:230: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  bert_feature = torch.load(path_bert, map_location=\"cpu\")\n",
            "/content/GPT-SoVITS/GPT_SoVITS/AR/data/dataset.py:230: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  bert_feature = torch.load(path_bert, map_location=\"cpu\")\n",
            "/content/GPT-SoVITS/GPT_SoVITS/AR/data/dataset.py:230: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  bert_feature = torch.load(path_bert, map_location=\"cpu\")\n",
            "/usr/local/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "/content/GPT-SoVITS/GPT_SoVITS/AR/data/dataset.py:230: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  bert_feature = torch.load(path_bert, map_location=\"cpu\")\n",
            "Epoch 9: 100% 15/15 [00:01<00:00,  7.63it/s, v_num=0, total_loss_step=192.0, lr_step=0.002, top_3_acc_step=0.994, total_loss_epoch=710.0, lr_epoch=0.002, top_3_acc_epoch=0.985]  `Trainer.fit` stopped: `max_epochs=10` reached.\n",
            "Epoch 9: 100% 15/15 [00:17<00:00,  1.14s/it, v_num=0, total_loss_step=192.0, lr_step=0.002, top_3_acc_step=0.994, total_loss_epoch=710.0, lr_epoch=0.002, top_3_acc_epoch=0.985]\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 2400, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/GPT-SoVITS/webui.py\", line 1048, in <module>\n",
            "    app.queue().launch(#concurrency_count=511, max_size=1022\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 2307, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 2404, in block_thread\n",
            "    self.server.close()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/http_server.py\", line 68, in close\n",
            "    self.thread.join()\n",
            "  File \"/usr/local/lib/python3.9/threading.py\", line 1060, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/local/lib/python3.9/threading.py\", line 1080, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 0.0.0.0:9874 <> https://1428dd1e9ae5238a18.gradio.live\n",
            "Killing tunnel 0.0.0.0:9871 <> https://6ea682d51316663fef.gradio.live\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}